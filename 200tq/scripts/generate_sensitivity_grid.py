# -*- coding: utf-8 -*-
"""
Sensitivity Grid Generator for OFF10 Experiments
=================================================

Generates cost/slippage sensitivity analysis for E00/E02/E03.

Grid:
- transaction_cost_bps âˆˆ {10, 20, 30, 50}
- slippage_bps âˆˆ {0, 5, 10, 20}
- tax_rate = 0.22 (fixed)

Author: QuantNeural v2026.1
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

# ============================================================
# PARAMETERS
# ============================================================
TAX_RATE = 0.22
TRADING_DAYS = 252

COST_GRID = [10, 20, 30, 50]  # bps
SLIPPAGE_GRID = [0, 5, 10, 20]  # bps

DEFAULT_EXPERIMENTS = ["E00_V0_Base_OFF10_CASH", "E02_V4_Ensemble_160_165_170", "E03_Ensemble_SGOV"]


# ============================================================
# LOAD EQUITY CURVES
# ============================================================
def load_equity_curves(experiments_dir: Path, exp_names: List[str]) -> Dict[str, pd.DataFrame]:
    """Load equity curves for specified experiments"""
    curves = {}
    
    for exp in exp_names:
        # Find matching directory
        for d in experiments_dir.iterdir():
            if exp in d.name and d.is_dir():
                eq_file = d / "equity_curve.csv"
                if eq_file.exists():
                    df = pd.read_csv(eq_file, parse_dates=["Date"], index_col="Date")
                    curves[exp] = df
                    print(f"   Loaded: {d.name}")
                break
    
    return curves


def load_trades(experiments_dir: Path, exp_names: List[str]) -> Dict[str, pd.DataFrame]:
    """Load trades for specified experiments"""
    trades = {}
    
    for exp in exp_names:
        for d in experiments_dir.iterdir():
            if exp in d.name and d.is_dir():
                trades_file = d / "trades.csv"
                if trades_file.exists():
                    df = pd.read_csv(trades_file, parse_dates=["Date"])
                    trades[exp] = df
                break
    
    return trades


# ============================================================
# SENSITIVITY CALCULATION
# ============================================================
def calculate_metrics_with_costs(equity: pd.Series, trades_df: pd.DataFrame,
                                  cost_bps: float, slippage_bps: float) -> Dict:
    """
    Re-calculate metrics with different cost/slippage assumptions.
    
    Approach: Apply additional cost drag to equity curve based on trade history.
    - Original equity has 10bps cost already applied
    - We adjust for the delta from baseline 10bps
    """
    # Get daily returns from equity
    returns = equity.pct_change().fillna(0.0)
    
    # Calculate additional cost drag
    # Trade notional is in the trades file - we need to adjust equity
    if len(trades_df) == 0:
        # No trades, just calculate metrics
        pass
    else:
        # Calculate cumulative cost difference
        # Original cost was 10bps, new cost is cost_bps + slippage_bps
        original_cost_bps = 10
        new_total_bps = cost_bps + slippage_bps
        delta_bps = new_total_bps - original_cost_bps
        
        if delta_bps != 0:
            # Apply additional cost drag on trade days
            trades_df = trades_df.copy()
            trades_df["Date"] = pd.to_datetime(trades_df["Date"])
            
            # Group by date to get total notional traded per day
            daily_notional = trades_df.groupby("Date")["notional"].sum()
            
            # Estimate daily cost drag as a fraction of portfolio
            # This is an approximation since we don't have exact portfolio value at each trade
            for dt, notional in daily_notional.items():
                if dt in returns.index:
                    # Cost drag = (delta_bps / 10000) * (notional / portfolio_value)
                    # Approximate portfolio value from equity
                    if dt in equity.index:
                        port_val = equity.loc[dt]
                        if port_val > 0:
                            cost_drag = (delta_bps / 10000.0) * (notional / port_val)
                            returns.loc[dt] -= cost_drag
    
    # Rebuild equity with adjusted returns
    adjusted_equity = (1 + returns).cumprod()
    
    # Calculate metrics
    n_years = len(adjusted_equity) / TRADING_DAYS
    final = float(adjusted_equity.iloc[-1])
    cagr = (final ** (1.0 / n_years) - 1.0) if n_years > 0 and final > 0 else 0.0
    
    peak = adjusted_equity.cummax()
    drawdown = adjusted_equity / peak - 1.0
    mdd = float(drawdown.min())
    
    daily_std = returns.std(ddof=0)
    sharpe = (returns.mean() / daily_std * np.sqrt(TRADING_DAYS)) if daily_std > 0 else 0.0
    
    return {
        "CAGR": cagr,
        "MDD": mdd,
        "Final": final,
        "Sharpe": sharpe,
    }


def generate_sensitivity_grid(experiments_dir: Path, exp_names: List[str],
                               cost_grid: List[int], slippage_grid: List[int]) -> pd.DataFrame:
    """Generate full sensitivity grid"""
    print("\nðŸ“Š Generating Sensitivity Grid...")
    
    # Load data
    curves = load_equity_curves(experiments_dir, exp_names)
    trades = load_trades(experiments_dir, exp_names)
    
    results = []
    
    for exp in exp_names:
        if exp not in curves:
            print(f"   âš ï¸ Skipping {exp}: no equity curve found")
            continue
        
        equity = curves[exp]["equity"]
        trades_df = trades.get(exp, pd.DataFrame())
        
        for cost_bps in cost_grid:
            for slip_bps in slippage_grid:
                metrics = calculate_metrics_with_costs(equity, trades_df, cost_bps, slip_bps)
                
                results.append({
                    "Experiment": exp,
                    "Cost_bps": cost_bps,
                    "Slippage_bps": slip_bps,
                    "Total_bps": cost_bps + slip_bps,
                    "CAGR": metrics["CAGR"],
                    "MDD": metrics["MDD"],
                    "Final": metrics["Final"],
                    "Sharpe": metrics["Sharpe"],
                })
    
    return pd.DataFrame(results)


def calculate_deltas(grid_df: pd.DataFrame, baseline: str) -> pd.DataFrame:
    """Calculate deltas vs baseline for each cost/slippage combo"""
    df = grid_df.copy()
    
    # Pivot to get baseline values
    base_mask = df["Experiment"].str.contains(baseline, case=False, na=False)
    base_df = df[base_mask].set_index(["Cost_bps", "Slippage_bps"])
    
    deltas = []
    
    for _, row in df.iterrows():
        key = (row["Cost_bps"], row["Slippage_bps"])
        
        if key in base_df.index:
            base_cagr = base_df.loc[key, "CAGR"]
            base_mdd = base_df.loc[key, "MDD"]
            
            deltas.append({
                "Experiment": row["Experiment"],
                "Cost_bps": row["Cost_bps"],
                "Slippage_bps": row["Slippage_bps"],
                "Total_bps": row["Total_bps"],
                "CAGR": row["CAGR"],
                "Î”CAGR": row["CAGR"] - base_cagr,
                "MDD": row["MDD"],
                "Î”MDD": row["MDD"] - base_mdd,
            })
        else:
            deltas.append({
                "Experiment": row["Experiment"],
                "Cost_bps": row["Cost_bps"],
                "Slippage_bps": row["Slippage_bps"],
                "Total_bps": row["Total_bps"],
                "CAGR": row["CAGR"],
                "Î”CAGR": 0.0,
                "MDD": row["MDD"],
                "Î”MDD": 0.0,
            })
    
    return pd.DataFrame(deltas)


def generate_verdict(delta_df: pd.DataFrame, candidate: str, baseline: str) -> Tuple[str, str]:
    """Generate verdict for sensitivity analysis"""
    cand_mask = delta_df["Experiment"].str.contains(candidate, case=False, na=False)
    cand_df = delta_df[cand_mask]
    
    if len(cand_df) == 0:
        return "UNKNOWN", "No candidate data found"
    
    # Find where Î”CAGR >= 0
    positive_delta = cand_df[cand_df["Î”CAGR"] >= 0]
    negative_delta = cand_df[cand_df["Î”CAGR"] < 0]
    
    total_scenarios = len(cand_df)
    positive_scenarios = len(positive_delta)
    
    # Find breakeven point
    breakeven = None
    for _, row in cand_df.sort_values("Total_bps").iterrows():
        if row["Î”CAGR"] < 0:
            breakeven = row["Total_bps"]
            break
    
    if positive_scenarios == total_scenarios:
        verdict = "ROBUST"
        msg = f"E03 maintains advantage across ALL cost/slippage scenarios ({total_scenarios}/{total_scenarios})"
    elif positive_scenarios >= total_scenarios * 0.75:
        verdict = "MOSTLY_ROBUST"
        msg = f"E03 maintains advantage in {positive_scenarios}/{total_scenarios} scenarios"
        if breakeven:
            msg += f". Advantage breaks at {breakeven}+ bps total."
    elif positive_scenarios >= total_scenarios * 0.5:
        verdict = "CONDITIONAL"
        msg = f"E03 advantage is marginal ({positive_scenarios}/{total_scenarios} scenarios)"
        if breakeven:
            msg += f". Breakeven point: {breakeven} bps."
    else:
        verdict = "NOT_ROBUST"
        msg = f"E03 advantage breaks under cost stress ({positive_scenarios}/{total_scenarios} scenarios)"
    
    return verdict, msg, breakeven


def main():
    parser = argparse.ArgumentParser(description="Generate sensitivity grid")
    parser.add_argument("--experiments-dir", type=str, default="200tq/experiments",
                        help="Path to experiments directory")
    parser.add_argument("--out-dir", type=str, default="200tq/artifacts/off10_robustness",
                        help="Output directory")
    parser.add_argument("--baseline", type=str, default="E00_V0_Base_OFF10_CASH")
    parser.add_argument("--candidate", type=str, default="E03_Ensemble_SGOV")
    
    args = parser.parse_args()
    
    experiments_dir = Path(args.experiments_dir)
    out_dir = Path(args.out_dir)
    
    print("="*80)
    print("       Sensitivity Grid Generator")
    print("="*80)
    
    # Generate grid
    grid_df = generate_sensitivity_grid(
        experiments_dir, 
        DEFAULT_EXPERIMENTS,
        COST_GRID,
        SLIPPAGE_GRID
    )
    
    # Calculate deltas
    delta_df = calculate_deltas(grid_df, args.baseline)
    
    # Generate verdict
    verdict, msg, breakeven = generate_verdict(delta_df, args.candidate, args.baseline)
    
    # Save outputs
    tables_dir = out_dir / "tables"
    notes_dir = out_dir / "notes"
    tables_dir.mkdir(parents=True, exist_ok=True)
    notes_dir.mkdir(parents=True, exist_ok=True)
    
    # Save grid CSV
    delta_df.to_csv(tables_dir / "sensitivity_grid.csv", index=False)
    print(f"\nâœ… Saved: {tables_dir}/sensitivity_grid.csv")
    
    # Generate verdict markdown
    verdict_md = f"""# Sensitivity Grid Analysis

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**Baseline**: {args.baseline}
**Candidate**: {args.candidate}

## Grid Parameters

- **Cost**: {COST_GRID} bps
- **Slippage**: {SLIPPAGE_GRID} bps
- **Tax**: {TAX_RATE*100:.0f}% (fixed)

## Verdict: **{verdict}**

{msg}

## Î”CAGR Summary (E03 vs E00)

| Cost (bps) | Slip (bps) | Total | Î”CAGR | Status |
|----------:|----------:|------:|------:|:------:|
"""
    
    cand_mask = delta_df["Experiment"].str.contains(args.candidate, case=False, na=False)
    for _, row in delta_df[cand_mask].sort_values("Total_bps").iterrows():
        status = "âœ…" if row["Î”CAGR"] >= 0 else "âš ï¸"
        verdict_md += f"| {row['Cost_bps']} | {row['Slippage_bps']} | {row['Total_bps']} | "
        verdict_md += f"{row['Î”CAGR']*100:+.2f}%p | {status} |\n"
    
    # Add breakeven info
    if breakeven:
        verdict_md += f"""
## Critical Finding

> **Breakeven Point**: ~{breakeven} bps total cost+slippage
> 
> E03's advantage over E00 begins to erode at this threshold.
> Current realistic cost (10 bps) + typical slippage (5-10 bps) = 15-20 bps â†’ **WITHIN SAFE ZONE**
"""
    else:
        verdict_md += f"""
## Critical Finding

> **No Breakeven Point Found**: E03 maintains advantage across all tested scenarios.
> Strategy is robust to cost/slippage stress up to 50+20=70 bps total.
"""
    
    with open(notes_dir / "sensitivity_grid_verdict.md", "w") as f:
        f.write(verdict_md)
    
    print(f"âœ… Saved: {notes_dir}/sensitivity_grid_verdict.md")
    
    # Print summary
    print("\n" + "="*80)
    print(f"   VERDICT: {verdict}")
    print(f"   {msg}")
    if breakeven:
        print(f"   Breakeven: {breakeven} bps")
    print("="*80)
    
    return delta_df, verdict, breakeven


if __name__ == "__main__":
    main()
